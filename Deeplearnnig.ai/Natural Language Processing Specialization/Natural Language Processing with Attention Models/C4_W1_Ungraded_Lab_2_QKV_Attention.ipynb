{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention: Ungraded Lab\n",
    "\n",
    "The 2017 paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) introduced the Transformer model and scaled dot-product attention, sometimes also called QKV (**Q**ueries, **K**eys, **V**alues) attention. Since then, Transformers have come to dominate large-scale natural language applications. Scaled dot-product attention can be used to improve seq2seq models as well. In this ungraded lab, you'll implement a simplified version of scaled dot-product attention and replicate word alignment between English and French, as shown in [Bhadanau, et al. (2014)](https://arxiv.org/abs/1409.0473).\n",
    "\n",
    "The Transformer model learns how to align words in different languages. You won't be training any weights here, so instead I've prepared some [pre-trained aligned word embeddings from here](https://fasttext.cc/docs/en/aligned-vectors.html). Run the cell below to load the embeddings and set up the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the word2int dictionaries\n",
    "with open(\"./data/word2int_en.pkl\", \"rb\") as f:\n",
    "    en_words = pickle.load(f)\n",
    "    \n",
    "with open(\"./data/word2int_fr.pkl\", \"rb\") as f:\n",
    "    fr_words = pickle.load(f)\n",
    "\n",
    "# Load the word embeddings\n",
    "en_embeddings = np.load(\"./data/embeddings_en.npz\")[\"embeddings\"]\n",
    "fr_embeddings = np.load(\"./data/embeddings_fr.npz\")[\"embeddings\"]\n",
    "\n",
    "def tokenize(sentence, token_mapping):\n",
    "    tokenized = []\n",
    "    \n",
    "    for word in sentence.lower().split(\" \"):\n",
    "        try:\n",
    "            tokenized.append(token_mapping[word])\n",
    "        except KeyError:\n",
    "            # Using -1 to indicate an unknown word\n",
    "            tokenized.append(-1)\n",
    "        \n",
    "    return tokenized\n",
    "\n",
    "def embed(tokens, embeddings):\n",
    "    embed_size = embeddings.shape[1]\n",
    "    \n",
    "    output = np.zeros((len(tokens), embed_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == -1:\n",
    "            output[i] = np.zeros((1, embed_size))\n",
    "        else:\n",
    "            output[i] = embeddings[token]\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled-dot product attention consists of two matrix multiplications and a softmax scaling as shown in the diagram below from [Vaswani, et al. (2017)](https://arxiv.org/abs/1706.03762). It takes three input matrices, the queries, keys, and values.\n",
    "\n",
    "![scaled-dot product attention diagram](./images/attention.png)\n",
    "\n",
    "Mathematically, this is expressed as\n",
    "\n",
    "$$ \n",
    "\\large \\mathrm{Attention}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where $Q$, $K$, and $V$ are the queries, keys, and values matrices respectively, and $d_k$ is the dimension of the keys. In practice, Q, K, and V all have the same dimensions. This form of attention is faster and more space-efficient than what you implemented before since it consists of only matrix multiplications instead of a learned feed-forward layer.\n",
    "\n",
    "Conceptually, the first matrix multiplication is a measure of the similarity between the queries and the keys. This is transformed into weights using the softmax function. These weights are then applied to the values with the second matrix multiplication resulting in output attention vectors. Typically, decoder states are used as the queries while encoder states are the keys and values.\n",
    "\n",
    "### Exercise 1\n",
    "Implement the softmax function with Numpy and use it to calculate the weights from the queries and keys. Assume the queries and keys are 2D arrays (matrices). Note that since the dot-product of Q and K will be a matrix, you'll need to take care to calculate softmax over a specific axis. See the end of the notebook for solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):    \n",
    "    \"\"\" Calculate softmax function for an array x\n",
    "\n",
    "        axis=0 calculates softmax across rows which means each column sums to 1 \n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "    # Replace pass with your code.\n",
    "    y = np.exp(x) \n",
    "    return y / np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "\n",
    "def calculate_weights(queries, keys):\n",
    "    \"\"\" Calculate the weights for scaled dot-product attention\"\"\"\n",
    "    # Replace None with your code.\n",
    "    dot = np.matmul(queries, keys.T)/ np.sqrt(keys.shape[1])\n",
    "    weights = softmax(dot, axis=1)\n",
    "    \n",
    "    assert weights.sum(axis=1)[0] == 1, \"Each row in weights must sum to 1\"\n",
    "    \n",
    "    # Replace pass with your code.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-131097cb3258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Visualize weights to check for alignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5523\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m    700\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0;32m--> 701\u001b[0;31m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize example sentences in English and French, then get their embeddings\n",
    "sentence_en = \"The agreement on the European Economic Area was signed in August 1992 .\"\n",
    "tokenized_en = tokenize(sentence_en, en_words)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "\n",
    "sentence_fr = \"L accord sur la zone économique européenne a été signé en août 1992 .\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_words)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "\n",
    "# These weights indicate alignment between words in English and French\n",
    "alignment = calculate_weights(embedded_fr, embedded_en)\n",
    "\n",
    "# Visualize weights to check for alignment\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.imshow(alignment, cmap='gray')\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticks(np.arange(alignment.shape[1]))\n",
    "ax.set_xticklabels(sentence_en.split(\" \"), rotation=90, size=16);\n",
    "ax.set_yticks(np.arange(alignment.shape[0]));\n",
    "ax.set_yticklabels(sentence_fr.split(\" \"), size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented the weights calculations correctly, the alignment matrix should look like this:\n",
    "\n",
    "![alignment visualization](./images/alignment.png)\n",
    "\n",
    "This is a demonstration of alignment where the model has learned which words in English correspond to words in French. For example, the words *signed* and *signé* have a large weight because they have the same meaning. Typically, these alignments are learned using linear layers in the model, but you've used pre-trained embeddings here.\n",
    "\n",
    "### Exercise 2\n",
    "Complete the implementation of scaled dot-product attention using your `calculate_weights` function (ignore the mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_qkv(queries, keys, values):\n",
    "    \"\"\" Calculate scaled dot-product attention from queries, keys, and values matrices \"\"\"\n",
    "    \n",
    "    # Replace pass with your code.\n",
    "    pass\n",
    "\n",
    "\n",
    "attention_qkv_result = attention_qkv(embedded_fr, embedded_en, embedded_en)\n",
    "\n",
    "print(f\"The shape of the attention_qkv function is {attention_qkv_result.shape}\")\n",
    "print(f\"Some elements of the attention_qkv function are \\n{attention_qkv_result[0:2,:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "The shape of the attention_qkv function is `(14, 300)`\n",
    "\n",
    "Some elements of the attention_qkv function are \n",
    "```python\n",
    "[[-0.04039161 -0.00275749  0.00389873  0.04842744 -0.02472726  0.01435613\n",
    "  -0.00370253 -0.0619686  -0.00206159  0.01615228]\n",
    " [-0.04083253 -0.00245985  0.00409068  0.04830341 -0.02479128  0.01447497\n",
    "  -0.00355203 -0.06196036 -0.00241327  0.01582606]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def softmax(x, axis=0):\n",
    "    \"\"\" Calculate softmax function for an array x\n",
    "    \n",
    "        axis=0 calculates softmax across rows which means each column sums to 1 \n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "    y = np.exp(x) \n",
    "    return y / np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "def calculate_weights(queries, keys):\n",
    "    \"\"\" Calculate the weights for scaled dot-product attention\"\"\"\n",
    "    dot = np.matmul(queries, keys.T)/np.sqrt(keys.shape[1])\n",
    "    weights = softmax(dot, axis=1)\n",
    "    \n",
    "    assert weights.sum(axis=1)[0] == 1, \"Each row in weights must sum to 1\"\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def attention_qkv(queries, keys, values):\n",
    "    \"\"\" Calculate scaled dot-product attention from queries, keys, and values matrices \"\"\"\n",
    "    weights = calculate_weights(queries, keys)\n",
    "    return np.matmul(weights, values)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
